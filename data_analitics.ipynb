{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1pxWIx_32NdXN7t7r8axTDD0FWIAiHFw0\n",
      "To: d:\\STUCOM\\Master_IABD\\Proyecto4---Recommendation\\data\\aisles.csv\n",
      "100%|██████████| 2.60k/2.60k [00:00<?, ?B/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1gMlH8PfLB5b8eVV1QuXI71GaxwpV_4EA\n",
      "To: d:\\STUCOM\\Master_IABD\\Proyecto4---Recommendation\\data\\departments.csv\n",
      "100%|██████████| 270/270 [00:00<00:00, 268kB/s]\n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1qR3CfKei5kacImYQz7x0xAypO4sm5H5O\n",
      "From (redirected): https://drive.google.com/uc?id=1qR3CfKei5kacImYQz7x0xAypO4sm5H5O&confirm=t&uuid=914d3601-e4e9-48e6-b8d6-ab468ec89cea\n",
      "To: d:\\STUCOM\\Master_IABD\\Proyecto4---Recommendation\\data\\orders.csv\n",
      "100%|██████████| 109M/109M [00:09<00:00, 11.3MB/s] \n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1nzd1C2Cc00854bDo7JznkIh9Q0zwIqyv\n",
      "From (redirected): https://drive.google.com/uc?id=1nzd1C2Cc00854bDo7JznkIh9Q0zwIqyv&confirm=t&uuid=9323d020-9dd7-41fa-a60b-9e086c246667\n",
      "To: d:\\STUCOM\\Master_IABD\\Proyecto4---Recommendation\\data\\order_products__prior.csv\n",
      "100%|██████████| 578M/578M [00:50<00:00, 11.5MB/s] \n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1vwOFeI8XWhe_ZlMb_r81jGwD0OKt8a2w\n",
      "To: d:\\STUCOM\\Master_IABD\\Proyecto4---Recommendation\\data\\order_products__train.csv\n",
      "100%|██████████| 24.7M/24.7M [00:02<00:00, 11.3MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1--mja_wkttg8BFLBxvQhxJ_jIAw1_9B3\n",
      "To: d:\\STUCOM\\Master_IABD\\Proyecto4---Recommendation\\data\\products.csv\n",
      "100%|██████████| 2.17M/2.17M [00:00<00:00, 8.74MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'data\\\\products.csv'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gdown\n",
    "\n",
    "# URL corregida (con uc?id=)\n",
    "url_aisle = \"https://drive.google.com/uc?id=1pxWIx_32NdXN7t7r8axTDD0FWIAiHFw0\"\n",
    "url_departments = \"https://drive.google.com/uc?id=1gMlH8PfLB5b8eVV1QuXI71GaxwpV_4EA\"\n",
    "url_orders = \"https://drive.google.com/uc?id=1qR3CfKei5kacImYQz7x0xAypO4sm5H5O\"\n",
    "url_orders_products_prior = \"https://drive.google.com/uc?id=1nzd1C2Cc00854bDo7JznkIh9Q0zwIqyv\"\n",
    "url_orders_products_train = \"https://drive.google.com/uc?id=1vwOFeI8XWhe_ZlMb_r81jGwD0OKt8a2w\"\n",
    "url_products = \"https://drive.google.com/uc?id=1--mja_wkttg8BFLBxvQhxJ_jIAw1_9B3\"\n",
    "\n",
    "\n",
    "# Descargar el archivo como aisles.csv\n",
    "gdown.download(url_aisle, r'data\\aisles.csv', quiet=False)\n",
    "gdown.download(url_departments, r'data\\departments.csv', quiet=False)\n",
    "gdown.download(url_orders, r'data\\orders.csv', quiet=False)\n",
    "gdown.download(url_orders_products_prior, r'data\\order_products__prior.csv', quiet=False)\n",
    "gdown.download(url_orders_products_train, r'data\\order_products__train.csv', quiet=False)\n",
    "gdown.download(url_products, r'data\\products.csv', quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Unknown error in IO callback",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 10\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Leer el CSV\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# df_aisles = pd.read_csv(r'data\\aisles.csv')\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# df_departments = pd.read_csv(r'data\\departments.csv')\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# df_order_products_train = pd.read_csv(r'data\\order_products__train.csv')\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# df_products = pd.read_csv(r'data\\products.csv')\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m df_merged \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mall_merged.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\joant\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\joant\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\joant\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1919\u001b[0m     (\n\u001b[0;32m   1920\u001b[0m         index,\n\u001b[0;32m   1921\u001b[0m         columns,\n\u001b[0;32m   1922\u001b[0m         col_dict,\n\u001b[1;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[0;32m   1924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[0;32m   1925\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\joant\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[1;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32mparsers.pyx:838\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:905\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:2061\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: Unknown error in IO callback"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Leer el CSV\n",
    "# df_aisles = pd.read_csv(r'data\\aisles.csv')\n",
    "# df_departments = pd.read_csv(r'data\\departments.csv')\n",
    "# df_orders = pd.read_csv(r'data\\orders.csv')\n",
    "# df_order_products_prior = pd.read_csv(r'data\\order_products__prior.csv')\n",
    "# df_order_products_train = pd.read_csv(r'data\\order_products__train.csv')\n",
    "# df_products = pd.read_csv(r'data\\products.csv')\n",
    "df_merged = pd.read_csv(r'data\\all_merged.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columnas de df_aisles: ['aisle_id', 'aisle']\n",
      "\n",
      "Columnas de df_departments: ['department_id', 'department']\n",
      "\n",
      "Columnas de df_orders: ['order_id', 'user_id', 'eval_set', 'order_number', 'order_dow', 'order_hour_of_day', 'days_since_prior_order']\n",
      "\n",
      "Columnas de df_order_products_prior: ['order_id', 'product_id', 'add_to_cart_order', 'reordered']\n",
      "\n",
      "Columnas de df_order_products_train: ['order_id', 'product_id', 'add_to_cart_order', 'reordered']\n",
      "\n",
      "Columnas de df_products: ['product_id', 'product_name', 'aisle_id', 'department_id']\n"
     ]
    }
   ],
   "source": [
    "print(\"Columnas de df_aisles:\", df_aisles.columns.tolist())\n",
    "print(\"\\nColumnas de df_departments:\", df_departments.columns.tolist())\n",
    "print(\"\\nColumnas de df_orders:\", df_orders.columns.tolist())\n",
    "print(\"\\nColumnas de df_order_products_prior:\", df_order_products_prior.columns.tolist())\n",
    "print(\"\\nColumnas de df_order_products_train:\", df_order_products_train.columns.tolist())\n",
    "print(\"\\nColumnas de df_products:\", df_products.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 134 entries, 0 to 133\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   aisle_id  134 non-null    int64 \n",
      " 1   aisle     134 non-null    object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 2.2+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 21 entries, 0 to 20\n",
      "Data columns (total 2 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   department_id  21 non-null     int64 \n",
      " 1   department     21 non-null     object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 464.0+ bytes\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3421083 entries, 0 to 3421082\n",
      "Data columns (total 7 columns):\n",
      " #   Column                  Dtype  \n",
      "---  ------                  -----  \n",
      " 0   order_id                int64  \n",
      " 1   user_id                 int64  \n",
      " 2   eval_set                object \n",
      " 3   order_number            int64  \n",
      " 4   order_dow               int64  \n",
      " 5   order_hour_of_day       int64  \n",
      " 6   days_since_prior_order  float64\n",
      "dtypes: float64(1), int64(5), object(1)\n",
      "memory usage: 182.7+ MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 32434489 entries, 0 to 32434488\n",
      "Data columns (total 4 columns):\n",
      " #   Column             Dtype\n",
      "---  ------             -----\n",
      " 0   order_id           int64\n",
      " 1   product_id         int64\n",
      " 2   add_to_cart_order  int64\n",
      " 3   reordered          int64\n",
      "dtypes: int64(4)\n",
      "memory usage: 989.8 MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1384617 entries, 0 to 1384616\n",
      "Data columns (total 4 columns):\n",
      " #   Column             Non-Null Count    Dtype\n",
      "---  ------             --------------    -----\n",
      " 0   order_id           1384617 non-null  int64\n",
      " 1   product_id         1384617 non-null  int64\n",
      " 2   add_to_cart_order  1384617 non-null  int64\n",
      " 3   reordered          1384617 non-null  int64\n",
      "dtypes: int64(4)\n",
      "memory usage: 42.3 MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 49688 entries, 0 to 49687\n",
      "Data columns (total 4 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   product_id     49688 non-null  int64 \n",
      " 1   product_name   49688 non-null  object\n",
      " 2   aisle_id       49688 non-null  int64 \n",
      " 3   department_id  49688 non-null  int64 \n",
      "dtypes: int64(3), object(1)\n",
      "memory usage: 1.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df_aisles.info()\n",
    "df_departments.info()\n",
    "df_orders.info()\n",
    "df_order_products_prior.info()\n",
    "df_order_products_train.info()\n",
    "df_products.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=df_orders['user_id'].unique(), columns=['user_id'])\n",
    "df.to_csv(r\"C:\\Users\\joant\\Downloads\\users.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REALZAR EL MERGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 33819106 entries, 0 to 33819105\n",
      "Data columns (total 10 columns):\n",
      " #   Column                  Dtype  \n",
      "---  ------                  -----  \n",
      " 0   order_id                int64  \n",
      " 1   user_id                 int64  \n",
      " 2   eval_set                object \n",
      " 3   order_number            int64  \n",
      " 4   order_dow               int64  \n",
      " 5   order_hour_of_day       int64  \n",
      " 6   days_since_prior_order  float64\n",
      " 7   product_id              int64  \n",
      " 8   add_to_cart_order       int64  \n",
      " 9   reordered               int64  \n",
      "dtypes: float64(1), int64(8), object(1)\n",
      "memory usage: 2.5+ GB\n"
     ]
    }
   ],
   "source": [
    "df_prior = df_orders.merge(df_order_products_prior, on=\"order_id\", how=\"inner\")\n",
    "df_train = df_orders.merge(df_order_products_train, on=\"order_id\", how=\"inner\")\n",
    "df_final = pd.concat([df_prior, df_train], ignore_index=True)\n",
    "\n",
    "df_final.info()\n",
    "\n",
    "df_final.to_csv(\"./data/merged_orders.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33819106, 10)\n",
      "   order_id  user_id eval_set  order_number  order_dow  order_hour_of_day  \\\n",
      "0   2539329        1    prior             1          2                  8   \n",
      "1   2539329        1    prior             1          2                  8   \n",
      "2   2539329        1    prior             1          2                  8   \n",
      "3   2539329        1    prior             1          2                  8   \n",
      "4   2539329        1    prior             1          2                  8   \n",
      "\n",
      "   days_since_prior_order  product_id  add_to_cart_order  reordered  \n",
      "0                     NaN         196                  1          0  \n",
      "1                     NaN       14084                  2          0  \n",
      "2                     NaN       12427                  3          0  \n",
      "3                     NaN       26088                  4          0  \n",
      "4                     NaN       26405                  5          0  \n"
     ]
    }
   ],
   "source": [
    "print(df_merged.shape)\n",
    "print(df_merged.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['order_id',\n",
       " 'user_id',\n",
       " 'eval_set',\n",
       " 'order_number',\n",
       " 'order_dow',\n",
       " 'order_hour_of_day',\n",
       " 'days_since_prior_order',\n",
       " 'product_id',\n",
       " 'add_to_cart_order',\n",
       " 'reordered']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged.columns.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MERGE DE TOTES LES TAULES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_products = df_products.merge(df_aisles, on='aisle_id', how='left')\n",
    "df_products = df_products.merge(df_departments, on='department_id', how='left')\n",
    "\n",
    "# Unir las órdenes con productos (prior y train)\n",
    "df_order_products = pd.concat([df_order_products_prior, df_order_products_train])\n",
    "\n",
    "# Unir los productos con sus detalles\n",
    "df_order_products = df_order_products.merge(df_products, on='product_id', how='left')\n",
    "\n",
    "# Unir con las órdenes\n",
    "df_final = df_order_products.merge(df_orders, on='order_id', how='left')\n",
    "\n",
    "df_final.to_csv(\"./data/all_merged.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>add_to_cart_order</th>\n",
       "      <th>reordered</th>\n",
       "      <th>aisle_id</th>\n",
       "      <th>department_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>order_number</th>\n",
       "      <th>order_dow</th>\n",
       "      <th>order_hour_of_day</th>\n",
       "      <th>days_since_prior_order</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3.381911e+07</td>\n",
       "      <td>3.381911e+07</td>\n",
       "      <td>3.381911e+07</td>\n",
       "      <td>3.381911e+07</td>\n",
       "      <td>3.381911e+07</td>\n",
       "      <td>3.381911e+07</td>\n",
       "      <td>3.381911e+07</td>\n",
       "      <td>3.381911e+07</td>\n",
       "      <td>3.381911e+07</td>\n",
       "      <td>3.381911e+07</td>\n",
       "      <td>3.174104e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.710566e+06</td>\n",
       "      <td>2.557551e+04</td>\n",
       "      <td>8.367738e+00</td>\n",
       "      <td>5.900617e-01</td>\n",
       "      <td>7.121799e+01</td>\n",
       "      <td>9.918544e+00</td>\n",
       "      <td>1.029444e+05</td>\n",
       "      <td>1.713998e+01</td>\n",
       "      <td>2.737285e+00</td>\n",
       "      <td>1.343123e+01</td>\n",
       "      <td>1.136415e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>9.874008e+05</td>\n",
       "      <td>1.409770e+04</td>\n",
       "      <td>7.139540e+00</td>\n",
       "      <td>4.918220e-01</td>\n",
       "      <td>3.819898e+01</td>\n",
       "      <td>6.281655e+00</td>\n",
       "      <td>5.946733e+04</td>\n",
       "      <td>1.749829e+01</td>\n",
       "      <td>2.093296e+00</td>\n",
       "      <td>4.246149e+00</td>\n",
       "      <td>8.940500e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>8.554130e+05</td>\n",
       "      <td>1.351900e+04</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>3.100000e+01</td>\n",
       "      <td>4.000000e+00</td>\n",
       "      <td>5.143500e+04</td>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+01</td>\n",
       "      <td>5.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.710660e+06</td>\n",
       "      <td>2.525600e+04</td>\n",
       "      <td>6.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>8.300000e+01</td>\n",
       "      <td>9.000000e+00</td>\n",
       "      <td>1.026260e+05</td>\n",
       "      <td>1.100000e+01</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>1.300000e+01</td>\n",
       "      <td>8.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.565587e+06</td>\n",
       "      <td>3.793500e+04</td>\n",
       "      <td>1.100000e+01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.070000e+02</td>\n",
       "      <td>1.600000e+01</td>\n",
       "      <td>1.544120e+05</td>\n",
       "      <td>2.400000e+01</td>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>1.600000e+01</td>\n",
       "      <td>1.500000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.421083e+06</td>\n",
       "      <td>4.968800e+04</td>\n",
       "      <td>1.450000e+02</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.340000e+02</td>\n",
       "      <td>2.100000e+01</td>\n",
       "      <td>2.062090e+05</td>\n",
       "      <td>1.000000e+02</td>\n",
       "      <td>6.000000e+00</td>\n",
       "      <td>2.300000e+01</td>\n",
       "      <td>3.000000e+01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           order_id    product_id  add_to_cart_order     reordered  \\\n",
       "count  3.381911e+07  3.381911e+07       3.381911e+07  3.381911e+07   \n",
       "mean   1.710566e+06  2.557551e+04       8.367738e+00  5.900617e-01   \n",
       "std    9.874008e+05  1.409770e+04       7.139540e+00  4.918220e-01   \n",
       "min    1.000000e+00  1.000000e+00       1.000000e+00  0.000000e+00   \n",
       "25%    8.554130e+05  1.351900e+04       3.000000e+00  0.000000e+00   \n",
       "50%    1.710660e+06  2.525600e+04       6.000000e+00  1.000000e+00   \n",
       "75%    2.565587e+06  3.793500e+04       1.100000e+01  1.000000e+00   \n",
       "max    3.421083e+06  4.968800e+04       1.450000e+02  1.000000e+00   \n",
       "\n",
       "           aisle_id  department_id       user_id  order_number     order_dow  \\\n",
       "count  3.381911e+07   3.381911e+07  3.381911e+07  3.381911e+07  3.381911e+07   \n",
       "mean   7.121799e+01   9.918544e+00  1.029444e+05  1.713998e+01  2.737285e+00   \n",
       "std    3.819898e+01   6.281655e+00  5.946733e+04  1.749829e+01  2.093296e+00   \n",
       "min    1.000000e+00   1.000000e+00  1.000000e+00  1.000000e+00  0.000000e+00   \n",
       "25%    3.100000e+01   4.000000e+00  5.143500e+04  5.000000e+00  1.000000e+00   \n",
       "50%    8.300000e+01   9.000000e+00  1.026260e+05  1.100000e+01  3.000000e+00   \n",
       "75%    1.070000e+02   1.600000e+01  1.544120e+05  2.400000e+01  5.000000e+00   \n",
       "max    1.340000e+02   2.100000e+01  2.062090e+05  1.000000e+02  6.000000e+00   \n",
       "\n",
       "       order_hour_of_day  days_since_prior_order  \n",
       "count       3.381911e+07            3.174104e+07  \n",
       "mean        1.343123e+01            1.136415e+01  \n",
       "std         4.246149e+00            8.940500e+00  \n",
       "min         0.000000e+00            0.000000e+00  \n",
       "25%         1.000000e+01            5.000000e+00  \n",
       "50%         1.300000e+01            8.000000e+00  \n",
       "75%         1.600000e+01            1.500000e+01  \n",
       "max         2.300000e+01            3.000000e+01  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## METRIQUES VARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Producto más vendido: 24852 con 491291 ventas\n",
      "Producto menos vendido: 3426 con 1 ventas\n"
     ]
    }
   ],
   "source": [
    "productos_vendidos = df_merged['product_id'].value_counts()\n",
    "\n",
    "# Producto más vendido\n",
    "producto_mas_vendido = productos_vendidos.idxmax()\n",
    "ventas_mas_vendido = productos_vendidos.max()\n",
    "\n",
    "# Producto menos vendido\n",
    "producto_menos_vendido = productos_vendidos.idxmin()\n",
    "ventas_menos_vendido = productos_vendidos.min()\n",
    "\n",
    "print(f\"Producto más vendido: {producto_mas_vendido} con {ventas_mas_vendido} ventas\")\n",
    "print(f\"Producto menos vendido: {producto_menos_vendido} con {ventas_menos_vendido} ventas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        order_hour_of_day  product_id  ventas\n",
      "11410                   0       24852    2915\n",
      "31970                   1       24852    1467\n",
      "48745                   2       24852     867\n",
      "62838                   3       24852     612\n",
      "76056                   4       24852     713\n",
      "90604                   5       24852    1368\n",
      "110523                  6       24852    4943\n",
      "138670                  7       24852   15060\n",
      "174072                  8       24852   28445\n",
      "213750                  9       24852   39352\n",
      "255581                 10       24852   42234\n",
      "298226                 11       24852   39931\n",
      "341063                 12       24852   37664\n",
      "383786                 13       24852   38445\n",
      "426703                 14       24852   39781\n",
      "469457                 15       24852   39700\n",
      "512000                 16       24852   38350\n",
      "553845                 17       24852   32019\n",
      "594422                 18       24852   25714\n",
      "633168                 19       24852   19625\n",
      "669600                 20       24852   15102\n",
      "703416                 21       24852   12155\n",
      "734824                 22       24852    9242\n",
      "763584                 23       24852    5587\n"
     ]
    }
   ],
   "source": [
    "# Contar productos vendidos por hora\n",
    "productos_por_hora = df_merged.groupby(['order_hour_of_day', 'product_id']).size().reset_index(name='ventas')\n",
    "\n",
    "# Obtener el producto más vendido por cada hora\n",
    "producto_mas_vendido_por_hora = productos_por_hora.loc[productos_por_hora.groupby('order_hour_of_day')['ventas'].idxmax()]\n",
    "\n",
    "print(producto_mas_vendido_por_hora)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((13176, 47209), 64761), ((13176, 21137), 64702), ((21137, 24852), 58330), ((24852, 47766), 55611), ((21903, 24852), 53395), ((13176, 21903), 52608), ((16797, 24852), 43180), ((24852, 47626), 43038), ((21137, 47209), 42333), ((13176, 27966), 42283)]\n"
     ]
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "from collections import Counter\n",
    "\n",
    "# Obtener lista de productos por cada orden\n",
    "ordenes = df_merged.groupby('order_id')['product_id'].apply(list)\n",
    "\n",
    "# Generar todas las combinaciones de pares de productos dentro de la misma orden\n",
    "pares_productos = [pair for lista in ordenes for pair in combinations(sorted(lista), 2)]\n",
    "\n",
    "# Contar las combinaciones más comunes\n",
    "top_10_pares = Counter(pares_productos).most_common(10)\n",
    "\n",
    "print(top_10_pares)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PARA LOS NULOS de days_since_prior_order\n",
    "SI UN CLIENTE TIENE MAS DE UN PEDIDO Y TIENE nan SE ELIMINA EL nan\n",
    "SI UN CLIENTE TIENE UN SOLO PEDIDO Y ES nan SE SUBSTITUYE POR 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Suponiendo que ya tienes el DataFrame cargado como 'orders_df'\n",
    "# orders_df = pd.read_csv('orders.csv')\n",
    "\n",
    "# Primero, vamos a comprobar cuántos pedidos tiene cada usuario\n",
    "pedido_por_usuario = df_orders.groupby('user_id')['order_id'].count()\n",
    "\n",
    "# Ahora, aplicamos la lógica según el número de pedidos de cada cliente\n",
    "for user_id, n_pedidos in pedido_por_usuario.items():\n",
    "    if n_pedidos > 1:\n",
    "        # Si tiene más de un pedido, eliminamos los NaN en 'days_since_prior_order' para ese usuario\n",
    "        df_orders.loc[df_orders['user_id'] == user_id, 'days_since_prior_order'] = \\\n",
    "            df_orders.loc[df_orders['user_id'] == user_id, 'days_since_prior_order'].dropna()\n",
    "    else:\n",
    "        # Si tiene solo un pedido, reemplazamos el NaN por 0\n",
    "        df_orders.loc[df_orders['user_id'] == user_id, 'days_since_prior_order'] = \\\n",
    "            df_orders.loc[df_orders['user_id'] == user_id, 'days_since_prior_order'].fillna(0)\n",
    "\n",
    "# Ahora ya hemos manejado los valores NaN según las condiciones establecidas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Agrupar por usuario para calcular métricas RFM\n",
    "rfm = df_orders.groupby('user_id').agg({\n",
    "    'order_number': 'max',  # Frecuencia (número de pedidos)\n",
    "    'days_since_prior_order': 'mean'  # Recencia (promedio de días entre pedidos)\n",
    "}).reset_index()\n",
    "\n",
    "# Renombrar columnas\n",
    "rfm.rename(columns={'order_number': 'frequency', 'days_since_prior_order': 'recency'}, inplace=True)\n",
    "\n",
    "# Normalizar valores para la segmentación\n",
    "scaler = StandardScaler()\n",
    "rfm_scaled = scaler.fit_transform(rfm[['recency', 'frequency']])\n",
    "\n",
    "# Calcular el RFM Score (si aún no lo has hecho)\n",
    "rfm['recency_score'] = rfm['recency'].rank(method='dense', ascending=False)\n",
    "rfm['frequency_score'] = rfm['frequency'].rank(method='dense', ascending=True)\n",
    "\n",
    "# Calcular el RFM total score combinando recencia y frecuencia\n",
    "rfm['RFM_Score'] = rfm['recency_score'] + rfm['frequency_score']\n",
    "\n",
    "# Definir los rangos de los segmentos utilizando np.where\n",
    "rfm['Segmento'] = np.where(rfm['RFM_Score'] >= 7, 'Clientes Leales', \n",
    "                            np.where(rfm['RFM_Score'] >= 5, 'Clientes Frecuentes', \n",
    "                                     np.where(rfm['RFM_Score'] >= 3, 'Clientes Regulares', \n",
    "                                              'Clientes Perdidos')))\n",
    "\n",
    "# Guardar el archivo CSV con los resultados\n",
    "rfm.to_csv(\"rfm_segmented_with_ranges.csv\", index=False)\n",
    "\n",
    "print(\"Archivo 'rfm_segmented_with_ranges.csv' generado con éxito.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Seleccionar características para la segmentación\n",
    "X = rfm[['recency', 'frequency']]\n",
    "\n",
    "# Escalar los datos\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Aplicar K-Means con 4 clusters\n",
    "kmeans = KMeans(n_clusters=4, random_state=42)\n",
    "rfm['cluster'] = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# Graficar los clusters\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=rfm['recency'], y=rfm['frequency'], hue=rfm['cluster'], palette='viridis', s=100)\n",
    "plt.xlabel(\"Recencia (Días desde última compra)\")\n",
    "plt.ylabel(\"Frecuencia (Cantidad de órdenes)\")\n",
    "plt.title(\"Segmentación de Clientes con K-Means\")\n",
    "plt.legend(title=\"Clusters\")\n",
    "plt.show()\n",
    "\n",
    "rfm.to_csv(\"data/kmeans_segmentation.csv\",index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
